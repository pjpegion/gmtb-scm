\chapter{Quick Start Guide}
\label{chapter: quick}

This chapter provides instructions for obtaining and compiling the GMTB SCM. The SCM code calls CCPP-compliant physics schemes through the CCPP framework code. As such, it requires the CCPP framework code and physics code, both of which are included as submodules within the SCM code. This package can be considered a simple example for an atmospheric model to interact with physics through the CCPP.

\section{Obtaining Code}

The source code for the CCPP and SCM is provided through GitHub.com.  This release branch contains the tested and supported version for general use.

\begin{enumerate}
    \item Clone the source using
\begin{lstlisting}[language=bash]
git clone --recursive -b v3.0 https://github.com/NCAR/gmtb-scm
\end{lstlisting}
             Recall that the \execout{recursive} option in this command clones the main gmtb-scm repository and all subrepositories (ccpp-physics and ccpp-framework). Using this option, there is no need to execute \exec{git submodule init} and \exec{git submodule update}.
    \item Change directory into the project.
\begin{lstlisting}[language=bash]
cd gmtb-scm
\end{lstlisting}
\end{enumerate}

The CCPP framework can be found in the ccpp/framework subdirectory at this level.  The CCPP physics parameterizations can be found in the ccpp/physics subdirectory.

If you would like to contribute as a developer to this project, please see (in addition to the rest of this guide) the scientific and technical documentation included with this release:

\url{https://dtcenter.org/community-code/common-community-physics-package-ccpp/documentation}

There you will find links to all of the documentation pertinent to developers.

For working with the development branches (stability not guaranteed), after executing the steps above, check out the \exec{dtc/develop} branches of the repository (and submodules):
\begin{enumerate}
    \item Check out the dtc/develop branch of gmtb-scm.
\begin{lstlisting}[language=bash]
git checkout dtc/develop
\end{lstlisting}
\item Check out the dtc/develop branches of ccpp-physics and ccpp-framework.
\begin{lstlisting}[language=bash]
cd ccpp/physics
git checkout dtc/develop
cd ../framework
git checkout dtc/develop
\end{lstlisting}
\item Change back to the main directory for following the instructions in section \ref{section: compiling} assuming system requirements in section \ref{section: systemrequirements} are met.
\begin{lstlisting}[language=bash]
cd ../..
\end{lstlisting}
\end{enumerate}


\section{System Requirements, Libraries, and Tools}
\label{section: systemrequirements}

The source code for the SCM and CCPP component is in the form of programs written in FORTRAN, FORTRAN 90, and C. In addition, the I/O relies on the netCDF libraries. Beyond the standard scripts, the build system relies on use of the Python scripting language, along with cmake, GNU make and date.

The basic requirements for building and running the CCPP and SCM bundle are listed below. The versions listed reflect successful tests and there is no guarantee that the code will work with different versions.
\begin{itemize}
    \item FORTRAN 90+ compiler
    	\begin{itemize}
   	 \item ifort 18.0.1.163 and 19.0.2
	 \item gfortran 6.2, 8.1, and 9.1
	 \item pgf90 17.7 and 17.9
	 \end{itemize}
    \item C compiler
    	\begin{itemize}
	\item icc 18.0.1.163 and 19.0.2
	\item gcc 6.2 and 8.1
	\item Apple Clang 10.0.0.10001145
	\item pgcc 17.7 and 17.9
	\end{itemize}
    \item cmake 2.8.12.1, 2.8.12.2, and 3.6.2
    \item netCDF 4.3.0, 4.4.0, 4.4.1.1, 4.5.0, 4.6.1 and 4.6.3 (not 3.x) with HDF5, ZLIB and SZIP
    \item Python 2.7.5, 2.7.9, and 2.7.13 (not 3.x)
    \item Libxml2 2.2, 2.9.7, 2.9.9
\end{itemize}

Because these tools and libraries are typically the purview of system administrators to install and maintain, they are considered  part of the basic system requirements.

Further, there are several utility libraries as part of the NCEPlibs package that must be installed prior to building the SCM. 
\begin{itemize}
    \item bacio - Binary I/O Library
    \item sp - Spectral Transformation Library
    \item w3nco - GRIB decoder and encoder library
\end{itemize}
These libraries are prebuilt on most NOAA machines using the Intel compiler. For those needing to build the libraries themselves, GMTB recommends using the source code from GitHub at \url{https://github.com/NCAR/NCEPlibs}, which includes build files for various compilers and machines using OpenMP flags and which are threadsafe. Instructions for installing NCEPlibs are included on the GitHub repository webpage, but for the sake of example, execute the following for obtaining and building from source in \execout{/usr/local/NCEPlibs} on a Mac:
\begin{lstlisting}
    mkdir /usr/local/NCEPlibs
    cd /usr/local/src
    git clone https://github.com/NCAR/NCEPlibs.git
    cd NCEPlibs
    ./make_ncep_libs.sh -s macosx -c gnu -d /usr/local/NCEPlibs -o 1 -m 0
\end{lstlisting}
Note that the option \execout{-m 0} can be used if MPI is not installed on the machine that is being used. The \execout{nemsio} library will not be installed, however, since it requires MPI. Once NCEPlibs is built, the \execout{NCEPLIBS\_DIR} environment variable must be set to the location of the installation. For example, if NCEPlibs was installed in \execout{/usr/local/NCEPlibs}, one would execute
\begin{lstlisting}
export NCEPLIBS_DIR=/usr/local/NCEPlibs
\end{lstlisting}
If using Hera or Cheyenne HPC systems, this environment variable is automatically set to an appropriate installation of NCEPlibs on those machines through use of one of the setup scripts described in section \ref{section: compiling}.


\subsection{Compilers}
The CCPP and SCM have been tested on a variety of
computing platforms. Currently the CCPP system is actively supported
on Linux and MacOS computing platforms using the Intel, PGI or GNU Fortran
compilers. Please use versions listed in the previous section as unforeseen
build issues may occur when using older compiler versions. Typically the best results come from using the
most recent version of a compiler. If you have problems with compilers, please check the ``Known Issues'' section of the
community website (\url{https://dtcenter.org/gmtb/users/ccpp/support/CCPP_KnownIssues.php}).

\section{Compiling SCM with CCPP}
\label{section: compiling}
The first step in compiling the CCPP and SCM is to properly setup your user environment. Platform-specific scripts are provided to load modules and set the user environment for common platforms.  If you are not using one of these platforms, you will need to set up the same environment on your platform. Following this step, the top level build system will use \execout{cmake} to query system parameters, execute the CCPP prebuild script to match the physics variables (between what the host model -- SCM -- can provide and what is needed by physics schemes in the CCPP), and build the physics caps needed to use them. Finally, \execout{make} is used to compile the components. 
\begin{enumerate}
 \item Change directory to the top-level SCM directory.
\begin{lstlisting}[language=bash]
cd scm
\end{lstlisting}
    \item (Optional) Run the machine setup script if necessary. This script loads compiler modules (Fortran 2003-compliant), netCDF module, etc. and sets compiler environment variables. For \textit{t/csh} shells,
\begin{lstlisting}[language=csh]
source etc/Hera_setup_intel.csh
source etc/Cheyenne_setup_gnu.csh
source etc/Cheyenne_setup_intel.csh
source etc/Cheyenne_setup_pgi.csh
source etc/UBUNTU_setup.csh
source etc/CENTOS_setup.csh
source etc/MACOSX_setup.csh
\end{lstlisting}
For bourne/bash shells,
\begin{lstlisting}[language=bash]
. etc/Hera_setup_intel.sh
. etc/Cheyenne_setup_gnu.sh
. etc/Cheyenne_setup_intel.sh
. etc/Cheyenne_setup_pgi.sh
. etc/UBUNTU_setup.sh
. etc/CENTOS_setup.sh
. etc/MACOSX_setup.sh
\end{lstlisting}
\emph{Note:} If using a local Linux or Mac system, we provide instructions for how to set up your development system (compilers and libraries) in \execout{doc/README\_\{MACOSX,UBUNTU,CENTOS\}.txt}. If following these, you will need to run the respective setup script listed above. If your computing environment was previously set up to use modern compilers with an associated netCDF installation, it may not be necessary, although we recommend setting environment variables such as \execout{CC} and \execout{FC}. \textbf{For version 3.0 and above, it is required to have the \execout{NETCDF} environment variable set to the path of the netCDF installation that was compiled with the same compiler used in the following steps}. Otherwise, the \execout{cmake} step will not complete successfully.

    \item Make a build directory and change into it.
\begin{lstlisting}[language=bash]
mkdir bin && cd bin
\end{lstlisting}

 \item Invoke cmake on the source code to build.
\begin{lstlisting}[language=bash]
cmake ../src                          # without threading/OpenMP
cmake -DOPENMP=ON ../src               # with threading/OpenMP
cmake -DCMAKE_BUILD_TYPE=Debug ../src # debug mode
cmake -DSTATIC=ON ../src.               #static mode
\end{lstlisting}
Cmake automatically runs the CCPP prebuild script to match required physics variables with those available from the dycore (SCM) and to generate physics caps and makefile segments. The default behavior is to use the so-called ``dynamic'' mode where software caps for all schemes in the CCPP repository are generated and a dynamic, shared library is generated for the CCPP physics. A second, more performant ``static'' mode is available that generates software caps for each physics group defined in the supplied SDFs and generates a static library that becomes part of the SCM executable. Using this mode, flexibility of defining suites at runtime is lost, although one can choose among a set of suites defined at compilation time. Adding the cmake variable \exec{-DSTATIC=ON} will turn this feature on and appropriate software caps will be generated for all suites defined in the \execout{gmtb-scm/ccpp/suites} directory automatically.

If necessary, the CCPP prebuild script can be executed manually from the top level directory (\execout{gmtb-scm}). The basic syntax for the dynamic mode is
\begin{lstlisting}[language=bash]
./ccpp/framework/scripts/ccpp_prebuild.py --config=./ccpp/config/ccpp_prebuild_config.py --builddir=./scm/bin [--debug]
\end{lstlisting}
To use the static mode, the syntax becomes
\begin{lstlisting}[language=bash]
./ccpp/framework/scripts/ccpp_prebuild.py --config=./ccpp/config/ccpp_prebuild_config.py --static --suites=SCM_GFS_v15p2,SCM_GFS_v16beta,SCM_GSD_v1[...] --builddir=./scm/bin [--debug]
\end{lstlisting}
where the argument supplied via the \execout{-{}-suites} variable is a comma-separated list of suite names that exist in the \execout{./ccpp/suites} directory. Note that suite names are the suite definition filenames minus the \exec{suite\_} prefix and \exec{.xml} suffix.
   
       \item If cmake cannot find \execout{libxml2} because it is installed in a non-standard location, add
\begin{lstlisting}[language=bash]
-DPC_LIBXML_INCLUDEDIR=... -DPC_LIBXML_LIBDIR=...
\end{lstlisting}
    to the cmake command. If a compilation error appears related to \execout{libxml2}, namely that a unicode file is not found in the include path, add the appropriate include path to the \execout{CFLAGS} environment variable
\begin{lstlisting}[language=bash]
export CFLAGS="-I/include_path"
\end{lstlisting} before cleaning out the \execout{bin} directory and rerunning \execout{cmake}.
    \item Compile. Add \execout{VERBOSE=1} to obtain more information on the build process.
\begin{lstlisting}[language=bash]
make
\end{lstlisting}
\end{enumerate}

The resulting executable may be found at \execout{./gmtb-scm} (Full path of \execout{gmtb-scm/scm/bin/gmtb-scm}). Depending on the system, it may be necessary to add the location of the CCPP framework and physics libraries to \execout{LD\_LIBRARY\_PATH} to run \execout{./gmtb-scm} (see next section).

Although \execout{make clean} is not currently implemented, an out-of-source build is used, so all that is required to clean the build/run directory is (from the \execout{bin} directory)
\begin{lstlisting}[language=bash]
pwd #confirm that you are in the gmtb-scm/scm/bin directory before deleting files
rm -rfd *
\end{lstlisting}
Note: This command can be dangerous (deletes files without confirming), so make sure that you're in the right directory before executing!

If you encounter errors, please capture a log file from all of the steps, and contact the helpdesk at: \url{gmtb-help@ucar.edu}

\section{Run the SCM with a supplied case}
There are several test cases provided with this version of the SCM. For all cases, the SCM will go through the time steps, applying forcing and calling the physics defined in the chosen suite definition file using physics configuration options from an associated namelist. The model is executed through one of two Python run scripts that are pre-staged into the \execout{bin} directory: \execout{run\_gmtb\_scm.py} or \execout{multi\_run\_gmtb\_scm.py}. The first sets up and runs one integration while the latter will set up and run several integrations serially. 

\subsection{Single Run Script Usage} \label{subsection: singlerunscript}
Running a case requires three pieces of information: the case to run (consisting of initial conditions, geolocation, forcing data, etc.), the physics suite to use (through a CCPP suite definition file), and a physics namelist (that specifies configurable physics options to use). As discussed in chapter \ref{chapter: cases}, cases are set up via their own namelists in \execout{../etc/case\_config}. A default physics suite is provided as a user-editable variable in the script and default namelists are associated with each physics suite (through \execout{../src/default\_namelists.py}), so, technically, one must only specify a case to run with the SCM. The single run script's interface is described below.

\begin{lstlisting}[language=bash]
./run_gmtb_scm.py -c CASE_NAME [-s SUITE_NAME] [-n PHYSICS_NAMELIST_WITH_PATH] [-g]
\end{lstlisting}

When invoking the run script, the only required argument is the name of the case to run. The case name used must match one of the case configuration files located in \execout{../etc/case\_config} (\emph{without the .nml extension!}). If specifying a suite other than the default, the suite name used must match the value of the suite name in one of the suite definition files located in \execout{../../ccpp/suites} (Note: not the filename of the suite definition file). As part of the third CCPP release, the following suite names are valid:
\begin{enumerate}
\item SCM\_GFS\_v15
\item SCM\_GFS\_v15plus
\item SCM\_csawmg
\item SCM\_GSD\_v0
\end{enumerate}

Note that using the Thompson microphysics scheme (as in SCM\_GSD\_v0) requires the computation of look-up tables during its initialization phase. As of the release, this process has been prohibitively slow with this model, so it is HIGHLY suggested that these look-up tables are downloaded and staged to use this scheme (and the SCM\_GSD\_v0 suite). Pre-computed tables have been created and are available for download at the following URLs:
\begin{itemize}
\item \url{https://dtcenter.org/GMTB/freezeH2O.dat} (243 M)
\item \url{https://dtcenter.org/GMTB/qr_acr_qg.dat} (49 M)
\item \url{https://dtcenter.org/GMTB/qr_acr_qs.dat} (32 M)
\end{itemize}
These files should be staged in \execout{gmtb-scm/scm/data/physics\_input\_data} prior to executing the run script. Since binary files can be system-dependent (due to endianness), it is possible that these files will not be read correctly on your system. For reference, the linked files were generated on Theia using the Intel v18 compiler.

Also note that some cases require specified surface fluxes. Special suite definition files that correspond to the suites listed above have been created and use the \execout{*\_prescribed\_surface} decoration. It is not necessary to specify this filename decoration when specifying the suite name. If the \execout{spec\_sfc\_flux} variable in the configuration file of the case being run is set to \execout{.true.}, the run script will automatically use the special suite definition file that corresponds to the chosen suite from the list above.

If specifying a namelist other than the default, the value must be an entire filename that exists in \execout{../../ccpp/physics\_namelists}. Caution should be exercised when modifying physics namelists since some redundancy between flags to control some physics parameterizations and scheme entries in the CCPP suite definition files currently exists. Values of numerical parameters are typically OK to change without fear of inconsistencies. Lastly, the \execout{-g} flag can be used to run the executable through the \exec{gdb} debugger (assuming it is installed on the system).

If the run aborts with the error message
\begin{lstlisting}[language=bash]
gmtb_scm: libccppphys.so.X.X.X: cannot open shared object file: No such file or directory
\end{lstlisting}
the environment variable \execout{LD\_LIBRARY\_PATH} must be set to
\begin{lstlisting}[language=bash]
export LD_LIBRARY_PATH=$PWD/ccpp/physics:$LD_LIBRARY_PATH
\end{lstlisting}
before running the model.

A netCDF output file is generated in the location specified in the case
configuration file, if the \execout{output\_dir} variable exists in that file. Otherwise an output directory is constructed from the case, suite, and namelist used (if different from the default). All output directories are placed in the \execout{bin} directory. Any standard netCDF file viewing or analysis tools may be used to
examine the output file (ncdump, ncview, NCL, etc).

\subsection{Multiple Run Script Usage}

A second Python script is provided for automating the execution of multiple integrations through repeated calling of the single run script. From the run directory, one may use this script through the following interface.

\begin{lstlisting}[language=bash]
./multi_run_gmtb_scm.py {[-c CASE_NAME] [-s SUITE_NAME] [-f PATH_TO_FILE]} [-v{v}] [-t]
\end{lstlisting}

No arguments are required for this script. The \execout{-c or --case}, \execout{-s or --suite}, or \execout{-f or --file} options form a mutually-exclusive group, so exactly one of these is allowed at one time. If \execout{--c} is specified with a case name, the script will run a set of integrations for all supported suites (defined in \execout{../src/supported\_suites.py}) for that case. If \execout{-s} is specified with a suite name, the script will run a set of integrations for all supported cases (defined in \execout{../src/supported\_cases.py}) for that that suite. If \execout{-f} is specified with the path to a filename, it will read in lists of cases, suites, and namelists to use from that file. An example for this file's syntax can be found in \execout{../src/example\_multi\_run.py}. If multiple namelists are specified in the file, there either must be one suite specified \emph{or} the number of suites must match the number of namelists. If none of the \execout{-c or --case}, \execout{-s or --suite}, or \execout{-f or --file} options group is specified, the script will run through all permutations of supported cases and suites (as defined in the files previously mentioned).

In addition to the main options, some helper options can also be used with any of those above. The \execout{-v{v} or --verbose} option can be used to output more information from the script to the console and to a log file. If this option is not used, only completion progress messages are written out. If one \execout{-v} is used, the script will write out completion progress messages and all messages and output from the single run script. If two \execout{-vv} are used, the script will also write out all messages and single run script output to a log file (\execout{multi\_run\_gmtb\_scm.log}) in the \execout{bin} directory. The final option, \execout{-t or --timer}, can be used to output the elapsed time for each integration executed by the script. Note that the execution time includes file operations performed by the single run script in addition to the execution of the underlying (Fortran) SCM executable. By default, this option will execute one integration of each subprocess. Since some variability is expected for each model run, if greater precision is required, the number of integrations for timing averaging can be set through an internal script variable. This option can be useful, for example, for getting a rough idea of relative computational expense of different physics suites.

\subsection{Batch Run Script}

If using the model on HPC resources and significant amounts of processor time is anticipated for the experiments, it will likely be necessary to submit a job through the HPC's batch system. An example script has been included in the repository for running the model on Hera's batch system (SLURM). It is located in \execout{gmtb-scm/scm/etc/gmtb\_scm\_slurm\_example.py}. Edit the \execout{job\_name}, \execout{account}, etc. to suit your needs and copy to the \execout{bin} directory. The case name to be run is included in the \execout{command} variable. To use, invoke
\begin{lstlisting}[language=bash]
./gmtb_scm_slurm_example.py
\end{lstlisting}
from the \execout{bin} directory.

Additional details regarding the SCM may be found in the remainder of this guide. More information on the CCPP can be found in the CCPP Technical Documentation available at \url{https://dtcenter.org/GMTB/v3.0/ccpp\_tech\_guide/}.
